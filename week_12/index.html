<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Module 12: Fairness and Responsible AI - DATASCI 207">
    <title>Module 12: Fairness and Responsible AI - DATASCI 207</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/styles.css">
</head>

<body>
    <header class="nav-header">
        <div class="container">
            <a href="../index.html" class="site-title">DATASCI 207: Applied Machine Learning</a>
        </div>
    </header>

    <main>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">Home</a>
                <span class="separator">/</span>
                <span>Module 12</span>
            </nav>

            <h1>Module 12: Fairness and Responsible AI</h1>

            <p>
                This module examines fairness, bias, and ethics in machine learning. You will learn
                how algorithmic systems can perpetuate or amplify societal biases, explore different
                definitions of fairness, and understand techniques for measuring and mitigating bias.
            </p>

            <div class="learning-objectives">
                <h3>Learning Objectives</h3>
                <ul>
                    <li>Identify sources of bias in ML systems (data, labels, features, algorithms).</li>
                    <li>Define and compare fairness criteria (demographic parity, equalized odds, calibration).</li>
                    <li>Explain why different fairness definitions can be mutually exclusive.</li>
                    <li>Measure fairness metrics across protected groups.</li>
                    <li>Apply bias mitigation techniques at different stages of the ML pipeline.</li>
                    <li>Discuss the broader societal implications of ML systems.</li>
                </ul>
            </div>

            <h2>Materials</h2>

            <nav class="module-nav">
                <a href="README.pdf">Module Summary</a>
                <a href="lecture.py">Lecture Code</a>
                <a href="glossary.html">Glossary</a>
                <a href="quiz.html">Knowledge Check</a>
                <a href="readings.html">Additional Readings</a>
                <a href="exercises/">Exercises</a>
                <a href="discussion.html">Discussion Topics</a>
            </nav>

            <h2>Key Concepts</h2>
            <ul>
                <li><strong>Algorithmic Bias:</strong> Systematic unfairness in predictions</li>
                <li><strong>Protected Attributes:</strong> Legally sensitive characteristics</li>
                <li><strong>Demographic Parity:</strong> Equal positive rates across groups</li>
                <li><strong>Equalized Odds:</strong> Equal TPR and FPR across groups</li>
                <li><strong>Fairness-Accuracy Tradeoff:</strong> Tension between objectives</li>
            </ul>

            <footer>
                <p>
                    <a href="../week_11/index.html">Previous: Module 11</a>
                    <span class="separator">|</span>
                    <a href="../index.html">Course Home</a>
                    <span class="separator">|</span>
                    <a href="../week_13/index.html">Next: Module 13</a>
                </p>
            </footer>
        </div>
    </main>
</body>

</html>