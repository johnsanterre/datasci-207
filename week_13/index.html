<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Module 13: Transformers and Attention - DATASCI 207">
    <title>Module 13: Transformers and Attention - DATASCI 207</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/styles.css">
</head>

<body>
    <header class="nav-header">
        <div class="container">
            <a href="../index.html" class="site-title">DATASCI 207: Applied Machine Learning</a>
        </div>
    </header>

    <main>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">Home</a>
                <span class="separator">/</span>
                <span>Module 13</span>
            </nav>

            <h1>Module 13: Transformers and Attention</h1>

            <p>
                This final module introduces the transformer architecture that powers modern AI systems
                like BERT and GPT. You will understand how attention mechanisms work, why they enable
                processing of long-range dependencies, and how to use pre-trained transformers for
                text classification and other NLP tasks.
            </p>

            <div class="learning-objectives">
                <h3>Learning Objectives</h3>
                <ul>
                    <li>Explain the limitations of recurrent networks that attention addresses.</li>
                    <li>Describe the intuition behind attention mechanisms.</li>
                    <li>Understand the Query-Key-Value formulation of self-attention.</li>
                    <li>Explain the complete transformer architecture.</li>
                    <li>Contrast encoder-only, decoder-only, and encoder-decoder models.</li>
                    <li>Fine-tune pre-trained transformers using Hugging Face.</li>
                </ul>
            </div>

            <h2>Materials</h2>

            <nav class="module-nav">
                <a href="README.pdf">Module Summary</a>
                <a href="lecture.py">Lecture Code</a>
                <a href="glossary.html">Glossary</a>
                <a href="quiz.html">Knowledge Check</a>
                <a href="readings.html">Additional Readings</a>
                <a href="exercises/">Exercises</a>
                <a href="discussion.html">Discussion Topics</a>
            </nav>

            <h2>Key Concepts</h2>
            <ul>
                <li><strong>Attention:</strong> Learn which parts of input to focus on</li>
                <li><strong>Self-Attention:</strong> Relate different positions within a sequence</li>
                <li><strong>Query-Key-Value:</strong> The attention computation framework</li>
                <li><strong>Transformer:</strong> Architecture based entirely on attention</li>
                <li><strong>Pre-training:</strong> BERT, GPT, and transfer learning</li>
            </ul>

            <footer>
                <p>
                    <a href="../week_12/index.html">Previous: Module 12</a>
                    <span class="separator">|</span>
                    <a href="../index.html">Course Home</a>
                    <span class="separator">|</span>
                    <a href="../capstone_best_practices.html">Capstone Guide</a>
                </p>
            </footer>
        </div>
    </main>
</body>

</html>