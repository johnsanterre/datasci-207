<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Module 6: Feedforward Neural Networks - DATASCI 207">
    <title>Module 6: Feedforward Neural Networks - DATASCI 207</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/styles.css">
</head>

<body>
    <header class="nav-header">
        <div class="container">
            <a href="../index.html" class="site-title">DATASCI 207: Applied Machine Learning</a>
        </div>
    </header>

    <main>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">Home</a>
                <span class="separator">/</span>
                <span>Module 6</span>
            </nav>

            <h1>Module 6: Feedforward Neural Networks</h1>

            <p>
                This module introduces neural networks as a way to learn non-linear decision boundaries.
                Starting with the XOR problem to motivate hidden layers, you will explore activation
                functions, network architecture, forward propagation, backpropagation, and practical
                training with TensorFlow/Keras.
            </p>

            <div class="learning-objectives">
                <h3>Learning Objectives</h3>
                <ul>
                    <li>Explain why hidden layers are necessary for learning non-linear patterns.</li>
                    <li>Describe the architecture of feedforward neural networks (layers, units, activations).</li>
                    <li>Compare activation functions: ReLU, sigmoid, and tanh.</li>
                    <li>Trace forward propagation through a network to compute predictions.</li>
                    <li>Explain the purpose of backpropagation for computing gradients.</li>
                    <li>Build and train neural networks using TensorFlow/Keras.</li>
                </ul>
            </div>

            <h2>Materials</h2>

            <nav class="module-nav">
                <a href="README.pdf">Module Summary</a>
                <a href="lecture.py">Lecture Code</a>
                <a href="glossary.html">Glossary</a>
                <a href="quiz.html">Knowledge Check</a>
                <a href="readings.html">Additional Readings</a>
                <a href="exercises/">Exercises</a>
                <a href="discussion.html">Discussion Topics</a>
            </nav>

            <h2>Key Concepts</h2>
            <ul>
                <li><strong>Hidden Layers:</strong> Enable learning non-linear patterns</li>
                <li><strong>Activation Functions:</strong> ReLU, sigmoid, tanh introduce non-linearity</li>
                <li><strong>Forward Propagation:</strong> Computing outputs from inputs</li>
                <li><strong>Backpropagation:</strong> Computing gradients for all parameters</li>
                <li><strong>Optimizers:</strong> SGD, Adam for updating weights</li>
            </ul>

            <footer>
                <p>
                    <a href="../week_05/index.html">Previous: Module 5</a>
                    <span class="separator">|</span>
                    <a href="../index.html">Course Home</a>
                    <span class="separator">|</span>
                    <a href="../week_07/index.html">Next: Module 7</a>
                </p>
            </footer>
        </div>
    </main>
</body>

</html>