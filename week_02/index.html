<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="description" content="Module 2: Linear Regression and Gradient Descent - DATASCI 207">
    <title>Module 2: Linear Regression and Gradient Descent - DATASCI 207</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/styles.css">
</head>

<body>
    <header class="nav-header">
        <div class="container">
            <a href="../index.html" class="site-title">DATASCI 207: Applied Machine Learning</a>
        </div>
    </header>

    <main>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">Home</a>
                <span class="separator">/</span>
                <span>Module 2</span>
            </nav>

            <h1>Module 2: Linear Regression and Gradient Descent</h1>

            <p>
                This module introduces learning by optimization in the context of the linear regression
                model. You will examine how the model fits a function to data by minimizing a loss
                function and how gradient descent serves as a key algorithm for updating parameters.
            </p>

            <div class="learning-objectives">
                <h3>Learning Objectives</h3>
                <ul>
                    <li>Define models, loss functions, and parameters in linear regression.</li>
                    <li>Describe learning by optimization and the purpose of gradient descent.</li>
                    <li>Apply gradient descent to minimize loss in linear regression.</li>
                    <li>Derive the update rule for gradient descent.</li>
                    <li>Compare batch, stochastic, and mini-batch gradient descent methods.</li>
                    <li>Evaluate how the learning rate and the loss shape affect convergence.</li>
                </ul>
            </div>

            <h2>Materials</h2>

            <nav class="module-nav">
                <a href="README.pdf">Module Summary</a>
                <a href="lecture.py">Lecture Code</a>
                <a href="glossary.html">Glossary</a>
                <a href="quiz.html">Knowledge Check</a>
                <a href="readings.html">Additional Readings</a>
                <a href="exercises/">Exercises</a>
                <a href="discussion.html">Discussion Topics</a>
            </nav>

            <h2>Key Concepts</h2>
            <ul>
                <li><strong>Linear Regression:</strong> Predicting a continuous output as a weighted sum of inputs</li>
                <li><strong>Loss Function:</strong> A measure of prediction error (e.g., Mean Squared Error)</li>
                <li><strong>Gradient Descent:</strong> Iterative optimization by following the negative gradient</li>
                <li><strong>Learning Rate:</strong> Step size for parameter updates</li>
                <li><strong>Convergence:</strong> When the optimization reaches a minimum</li>
            </ul>

            <h2>Central Concepts from Prerequisites</h2>
            <ul>
                <li>Calculus: derivatives (rate of change of a function)</li>
            </ul>

            <footer>
                <p>
                    <a href="../week_01/index.html">Previous: Module 1</a>
                    <span class="separator">|</span>
                    <a href="../index.html">Course Home</a>
                    <span class="separator">|</span>
                    <a href="../week_03/index.html">Next: Module 3</a>
                </p>
            </footer>
        </div>
    </main>
</body>

</html>