<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Quiz: Module 9 - DATASCI 207</title>
    <link rel="preconnect" href="https://fonts.googleapis.com">
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@400;500;600&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="../assets/styles.css">
</head>

<body>
    <header class="nav-header">
        <div class="container">
            <a href="../index.html" class="site-title">DATASCI 207: Applied Machine Learning</a>
        </div>
    </header>

    <main>
        <div class="container">
            <nav class="breadcrumb">
                <a href="../index.html">Home</a>
                <span class="separator">/</span>
                <a href="index.html">Module 9</a>
                <span class="separator">/</span>
                <span>Knowledge Check</span>
            </nav>

            <h1>Module 9: Knowledge Check</h1>

            <div class="quiz-score" id="quiz-score">
                <span class="score-label">Questions</span>
                <span class="score-value">5</span>
            </div>

            <div class="quiz-container">

                <div class="quiz-question" data-correct="c" data-explanations='{
                    "a": "IDF is computed once per term across the entire corpus, not per document.",
                    "b": "Lower document frequency means IDF will be HIGHER (more rare = more important).",
                    "c": "Correct. Words appearing in many documents have high DF and thus low IDF. This down-weights common terms like \"the\" and \"and\" that appear everywhere.",
                    "d": "IDF relates to document frequency, not word length."
                }'>
                    <span class="question-number">Question 1</span>
                    <h4>What does the IDF component of TF-IDF accomplish?</h4>
                    <ul class="quiz-options">
                        <li><label><input type="radio" name="q1" value="a"> It increases scores for words appearing in
                                many documents</label></li>
                        <li><label><input type="radio" name="q1" value="b"> It gives higher weight to words with lower
                                document frequency</label></li>
                        <li><label><input type="radio" name="q1" value="c"> It down-weights common words that appear in
                                many documents</label></li>
                        <li><label><input type="radio" name="q1" value="d"> It removes stop words from the
                                vocabulary</label></li>
                    </ul>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-question" data-correct="b" data-explanations='{
                    "a": "Bag-of-words explicitly ignores word order by design.",
                    "b": "Correct. BoW treats \"dog bites man\" and \"man bites dog\" identically because it only counts occurrences, not order. This is a significant limitation.",
                    "c": "BoW preserves exact counts, not just binary presence (though binary variants exist).",
                    "d": "BoW works fine with varying document lengths (normalization can address length differences)."
                }'>
                    <span class="question-number">Question 2</span>
                    <h4>What is a major limitation of the bag-of-words representation?</h4>
                    <ul class="quiz-options">
                        <li><label><input type="radio" name="q2" value="a"> It preserves word order</label></li>
                        <li><label><input type="radio" name="q2" value="b"> It ignores word order completely</label>
                        </li>
                        <li><label><input type="radio" name="q2" value="c"> It cannot count word frequencies</label>
                        </li>
                        <li><label><input type="radio" name="q2" value="d"> It only works with fixed-length
                                documents</label></li>
                    </ul>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-question" data-correct="d" data-explanations='{
                    "a": "Word embeddings are dense, not sparse like one-hot encoding.",
                    "b": "Word embeddings require significant training time or using pre-trained models (not simpler).",
                    "c": "Embedding dimensions are fixed regardless of vocabulary size (typically 100-300D).",
                    "d": "Correct. Words appearing in similar contexts get similar vectors during training. This means synonyms like \"happy\" and \"joyful\" end up close together in embedding space."
                }'>
                    <span class="question-number">Question 3</span>
                    <h4>Why can word embeddings capture semantic similarity?</h4>
                    <ul class="quiz-options">
                        <li><label><input type="radio" name="q3" value="a"> They use one-hot encoding</label></li>
                        <li><label><input type="radio" name="q3" value="b"> They are simpler than bag-of-words</label>
                        </li>
                        <li><label><input type="radio" name="q3" value="c"> They have as many dimensions as vocabulary
                                size</label></li>
                        <li><label><input type="radio" name="q3" value="d"> Words in similar contexts are trained to
                                have similar vectors</label></li>
                    </ul>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-question" data-correct="a" data-explanations='{
                    "a": "Correct. Averaging word embeddings is simple but effective. Each word contributes its vector, and the mean captures the overall semantic content. TF-IDF weighting can improve this further.",
                    "b": "Concatenation would produce variable-length vectors, which is problematic.",
                    "c": "Selecting only certain words might miss important content.",
                    "d": "Individual word embeddings have fixed dimensions; the challenge is combining them."
                }'>
                    <span class="question-number">Question 4</span>
                    <h4>What is a simple way to create a document embedding from word embeddings?</h4>
                    <ul class="quiz-options">
                        <li><label><input type="radio" name="q4" value="a"> Average the embeddings of all words in the
                                document</label></li>
                        <li><label><input type="radio" name="q4" value="b"> Concatenate all word embeddings
                                together</label></li>
                        <li><label><input type="radio" name="q4" value="c"> Use only the first word's embedding</label>
                        </li>
                        <li><label><input type="radio" name="q4" value="d"> Increase the embedding dimensions</label>
                        </li>
                    </ul>
                    <div class="quiz-feedback"></div>
                </div>

                <div class="quiz-question" data-correct="c" data-explanations='{
                    "a": "Cosine similarity ranges from -1 to 1, not 0 to 1 exclusively.",
                    "b": "The dot product of normalized vectors equals cosine (magnitude-independent, not count-based).",
                    "c": "Correct. Cosine similarity measures the angle between vectors. A cosine of 1 means the vectors point in exactly the same direction (parallel), indicating high similarity.",
                    "d": "Cosine uses the angle (direction), not the distance (length) between vectors."
                }'>
                    <span class="question-number">Question 5</span>
                    <h4>What does a cosine similarity of 1 indicate about two vectors?</h4>
                    <ul class="quiz-options">
                        <li><label><input type="radio" name="q5" value="a"> They have the same magnitude</label></li>
                        <li><label><input type="radio" name="q5" value="b"> They share the same word counts</label></li>
                        <li><label><input type="radio" name="q5" value="c"> They point in exactly the same
                                direction</label></li>
                        <li><label><input type="radio" name="q5" value="d"> They have the smallest distance between
                                them</label></li>
                    </ul>
                    <div class="quiz-feedback"></div>
                </div>

            </div>

            <div class="quiz-actions">
                <button class="btn btn-secondary" id="reset-quiz">Reset Quiz</button>
            </div>

            <footer>
                <p><a href="index.html">Back to Module 9</a></p>
            </footer>
        </div>
    </main>

    <script src="../assets/quiz.js"></script>
</body>

</html>